{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is a popular deep learning framework and it's easy to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: torchvision in d:\\anaconda\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.19.2)\n",
      "Requirement already satisfied: torch==1.8.1 in d:\\anaconda\\lib\\site-packages (from torchvision) (1.8.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\anaconda\\lib\\site-packages (from torchvision) (8.0.1)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch==1.8.1->torchvision) (3.7.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gzip\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the mnist data, preprocess them and encapsulate them into dataloader form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "332551e02d9446c78c6f92c774cf7294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=9912422.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./mnist/MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388ea29f7dde40b2801d981252563854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=28881.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./mnist/MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bf755c27014b86ba42dc2081475052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1648877.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./mnist/MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 503: Service Unavailable\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d0658992e664d58b1c389c5382084a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=4542.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting ./mnist/MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist/MNIST\\raw\n",
      "\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:502: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:143.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "normalize = transforms.Normalize(mean=[.5], std=[.5])\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "\n",
    "# download and load the data\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist/', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist/', train=False, transform=transform, download=False)\n",
    "\n",
    "# encapsulate them into dataloader form\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define the model, object function and optimizer that we use to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO:define model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet,self).__init__()\n",
    "        #self.fc1构建第一个全连接层（full connect），SimpleNet.Linear是torch的全连接层方法，两个参数是左右的神经元个数\n",
    "        self.fc1 = nn.Linear(28*28,500)\n",
    "        self.fc2 = nn.Linear(500,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        #F.relu是torch的relu激活函数，还有其他激活函数，可以参考官网\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "model = SimpleNet()\n",
    "# TODO:define loss function and optimiter\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr = 0.01,momentum = 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can start to train and evaluate!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0031\n",
      "Epoch [1/10], Loss: 0.0016\n",
      "Epoch [1/10], Loss: 0.0050\n",
      "Epoch [1/10], Loss: 0.0031\n",
      "Epoch [1/10], Loss: 0.0037\n",
      "Epoch [2/10], Loss: 0.0014\n",
      "Epoch [2/10], Loss: 0.0011\n",
      "Epoch [2/10], Loss: 0.0014\n",
      "Epoch [2/10], Loss: 0.0019\n",
      "Epoch [2/10], Loss: 0.0028\n",
      "Epoch [3/10], Loss: 0.0021\n",
      "Epoch [3/10], Loss: 0.0026\n",
      "Epoch [3/10], Loss: 0.0033\n",
      "Epoch [3/10], Loss: 0.0024\n",
      "Epoch [3/10], Loss: 0.0028\n",
      "Epoch [4/10], Loss: 0.0027\n",
      "Epoch [4/10], Loss: 0.0015\n",
      "Epoch [4/10], Loss: 0.0046\n",
      "Epoch [4/10], Loss: 0.0013\n",
      "Epoch [4/10], Loss: 0.0005\n",
      "Epoch [5/10], Loss: 0.0028\n",
      "Epoch [5/10], Loss: 0.0025\n",
      "Epoch [5/10], Loss: 0.0009\n",
      "Epoch [5/10], Loss: 0.0026\n",
      "Epoch [5/10], Loss: 0.0022\n",
      "Epoch [6/10], Loss: 0.0030\n",
      "Epoch [6/10], Loss: 0.0016\n",
      "Epoch [6/10], Loss: 0.0011\n",
      "Epoch [6/10], Loss: 0.0026\n",
      "Epoch [6/10], Loss: 0.0020\n",
      "Epoch [7/10], Loss: 0.0022\n",
      "Epoch [7/10], Loss: 0.0013\n",
      "Epoch [7/10], Loss: 0.0021\n",
      "Epoch [7/10], Loss: 0.0026\n",
      "Epoch [7/10], Loss: 0.0024\n",
      "Epoch [8/10], Loss: 0.0017\n",
      "Epoch [8/10], Loss: 0.0022\n",
      "Epoch [8/10], Loss: 0.0036\n",
      "Epoch [8/10], Loss: 0.0033\n",
      "Epoch [8/10], Loss: 0.0010\n",
      "Epoch [9/10], Loss: 0.0005\n",
      "Epoch [9/10], Loss: 0.0006\n",
      "Epoch [9/10], Loss: 0.0026\n",
      "Epoch [9/10], Loss: 0.0014\n",
      "Epoch [9/10], Loss: 0.0029\n",
      "Epoch [10/10], Loss: 0.0009\n",
      "Epoch [10/10], Loss: 0.0019\n",
      "Epoch [10/10], Loss: 0.0004\n",
      "Epoch [10/10], Loss: 0.0016\n",
      "Epoch [10/10], Loss: 0.0020\n"
     ]
    }
   ],
   "source": [
    "# train and evaluate\n",
    "loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "      for i, data in enumerate(train_loader):\n",
    "        (inputs, labels) =  data\n",
    "        #step1，获取inputs，计算outputs\n",
    "        #因为是全连接网络，所以需要reshape到一维，卷积不需要\n",
    "        inputs = inputs.reshape(-1, 28*28)\n",
    "        outputs = model(inputs)\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum()\n",
    "        #step2，清零梯度，计算loss，反向传播\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i%100 == 0:\n",
    "                print('Epoch [{}/{}], Loss: {:.4f}'\n",
    "                      .format(epoch + 1, NUM_EPOCHS, loss.item())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5:\n",
    "Please print the training and testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 training accuracy:99.99883270263672\n",
      "Epoch 10 testing accuracy:98.28726196289062\n"
     ]
    }
   ],
   "source": [
    "def test(test_loader,model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.reshape(-1, 28*28)\n",
    "        outputs = model(inputs)\n",
    "        #torch.max并不是np.max一个意思，是用以计算sofamax的分类类别的，建议CSDN查一下\n",
    "        _, predict = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predict == labels).sum()\n",
    "    print('Epoch {} testing accuracy:{}'.format(epoch+1, 100*correct/total))\n",
    "\n",
    "print('Epoch {} training accuracy:{}'.format(epoch+1, 100*correct/total))\n",
    "test(test_loader,model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
